{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/9095-advocate\">Advocate</a>\n",
      "<a href=\"/9033-love-and-sympathy\">Love and Sympathy</a>\n",
      "<a href=\"/9031-do-you-remember-love\">Do You Remember Love</a>\n",
      "<a href=\"/9023-mimang\">Mimang</a>\n",
      "<a href=\"/9021-the-lie\">The Lie</a>\n",
      "<a href=\"/9016-legends-of-love\">Legends of Love</a>\n",
      "<a href=\"/9015-swat-police\">SWAT Police</a>\n",
      "<a href=\"/9014-dawn-of-the-empire\">Dawn of the Empire</a>\n",
      "<a href=\"/9004-my-sons-woman\">My Son's Woman</a>\n",
      "<a href=\"/9003-crush\">Crush</a>\n",
      "<a href=\"/8887-swan-lake\">Swan Lake</a>\n",
      "<a href=\"/8846-company-love\">Company Love</a>\n",
      "<a href=\"/8435-there-is-a-blue-bird\">There Is a Blue Bird</a>\n",
      "<a href=\"/8424-blue-tower-zero\">Blue Tower ZERO</a>\n",
      "<a href=\"/7462-a-pioneer-lee-che-ma\">A Pioneer, Lee Che Ma</a>\n",
      "<a href=\"/7457-hail\">Hail</a>\n",
      "<a href=\"/6979-last-lovers\">Last Lovers</a>\n",
      "<a href=\"/6977-their-embrace\">Their Embrace </a>\n",
      "<a href=\"/6976-the-moon-of-seoul\">The Moon of Seoul </a>\n",
      "<a href=\"/6511-heart-of-lies\">Heart of Lies</a>\n",
      "<a href=\"/6239-blue-fog\">Blue Fog</a>\n",
      "<a href=\"/5898-trio\">Trio</a>\n",
      "<a href=\"/5897-three-wives\">Three Wives</a>\n",
      "<a href=\"/5896-three-men\">Three Men</a>\n",
      "<a href=\"/5890-the-devil-that-pours-red-wine\">The Devil That Pours Red Wine</a>\n",
      "<a href=\"/5886-special-of-my-life\">Special of My Life</a>\n",
      "<a href=\"/5881-sa-yug-shin\">Six Martyred Ministers</a>\n",
      "<a href=\"/5870-pretty-woman-2003\">Pretty Woman</a>\n",
      "<a href=\"/5856-i-live-without-incident\">I Live Without Incident</a>\n",
      "<a href=\"/5855-im-from-chosun\">I'm From Chosun</a>\n",
      "<a href=\"/5847-good-news\">Good News</a>\n",
      "<a href=\"/5846-face-me-and-smile\">Face Me and Smile</a>\n",
      "<a href=\"/5831-bad-girls\">Bad Girls</a>\n",
      "<a href=\"/5793-short-family\">Short Family</a>\n",
      "<a href=\"/5744-moms-song\">Mom's Song</a>\n",
      "<a href=\"/5603-yes-sir\">Yes, Sir </a>\n",
      "<a href=\"/5523-everyday-with-you\">Everyday With You</a>\n",
      "<a href=\"/5516-kokkiri\">Kokkiri</a>\n",
      "<a href=\"/5515-common-single\">Common Single</a>\n",
      "<a href=\"/5442-a-bluebird-has-it\">A Bluebird Has It</a>\n",
      "https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&so=top&page=160\n",
      "<a href=\"/5441-ready-go\">Ready Go! </a>\n",
      "<a href=\"/5440-man-in-crisis\">Man In Crisis</a>\n",
      "<a href=\"/4527-woman-on-top\">I'll Show You the Taste</a>\n",
      "<a href=\"/3837-han-myung-hoe\">Han Myung Hoe </a>\n",
      "<a href=\"/3834-hotel\">Hotel </a>\n",
      "<a href=\"/3833-spider\">Spider</a>\n",
      "<a href=\"/3510-the-king-of-chudong-palace\">The King of Chudong Palace </a>\n",
      "<a href=\"/1829-lottery-trio\">Lottery Trio</a>\n",
      "<a href=\"/1504-one-day-suddenly\">One Day Suddenly</a>\n",
      "<a href=\"/1459-scary-girl\">Scary Girl</a>\n",
      "<a href=\"/1098-i-love-you-my-enemy\">I Love You, My Enemy</a>\n",
      "<a href=\"/1093-legend-of-ambition\">Legend of Ambition</a>\n",
      "https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&so=top&page=161\n",
      "https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&so=top&page=162\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Gets all links from kdrama site \n",
    "@author kayak, Prof.K\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import requests  # library to send HTTP requests\n",
    "from bs4 import BeautifulSoup # gets stuff from the website\n",
    "import re # regex\n",
    "\n",
    "url = 'https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&so=top&page=159' # kdrama site url\n",
    "links = [] # links of all kdrama\n",
    "\n",
    "\n",
    "\"\"\"Checks regex\n",
    "\n",
    "if regex doesn't match, returns -1; else returns the page number + 1\n",
    "\"\"\"\n",
    "def checkRegex(regex, pageNum):\n",
    "    if bool(re.search(regex, pageNum)):\n",
    "        newPageNum = int(pageNum) + 1\n",
    "        return str(newPageNum)\n",
    "    return -1\n",
    "\n",
    "\"\"\"Updates link to the next page url\n",
    "\"\"\"\n",
    "def updateLink(URL):\n",
    "    # PN = page number\n",
    "    threeDigitPN = URL[-3:]\n",
    "    regexThreeDigit = \"\\\\d{3}\"\n",
    "    twoDigitPN = URL[-2:]\n",
    "    regexTwoDigit = \"\\\\d{2}\"\n",
    "    oneDigitPN = URL[-1:]\n",
    "    regexOneDigit = \"\\\\d{1}\"\n",
    "\n",
    "    if checkRegex(regexThreeDigit, threeDigitPN) != -1:\n",
    "        return URL[:-3] + checkRegex(regexThreeDigit, threeDigitPN)\n",
    "\n",
    "    if checkRegex(regexTwoDigit, twoDigitPN) != -1:\n",
    "        return URL[:-2] + checkRegex(regexTwoDigit, twoDigitPN)\n",
    "\n",
    "    if checkRegex(regexOneDigit, oneDigitPN) != -1:\n",
    "        return URL[:-1] + checkRegex(regexOneDigit, oneDigitPN)\n",
    "\n",
    "    \"\"\"add links to link list from this URL\n",
    "    \"\"\"\n",
    "def addLinks(url):\n",
    "    response = requests.get(url) # sends and get HTTP response\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    kdramaSites = soup.select('div > div.col-xs-9.row-cell.content > h6 > a:nth-child(1)') # list of all items with this selector\n",
    "\n",
    "    for site in kdramaSites:\n",
    "        print(site)\n",
    "        links.append(\"https://mydramalist.com\" + site.get('href'))\n",
    "\n",
    "    \"\"\"Adds all links from the mydramalist to links list\n",
    "    \"\"\"\n",
    "def addAllLinks(url):\n",
    "    beforeURL = links[-1]\n",
    "    addLinks(url)\n",
    "    afterURL = links[-1]\n",
    "\n",
    "    # last page\n",
    "    if (beforeURL == afterURL): return\n",
    "\n",
    "    addAllLinks(updateLink(url))\n",
    "\n",
    "# def addAllLinks(url, pageNum):\n",
    "#     if (pageNum == 5): return\n",
    "\n",
    "#     print(len(links))\n",
    "\n",
    "#     pageNum += 1\n",
    "#     addLinks(url)\n",
    "\n",
    "#     print(len(links))\n",
    "#     addAllLinks(updateLink(url), pageNum)\n",
    "\n",
    "\n",
    "# addLinks(\"https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&so=top&page=202\")\n",
    "\n",
    "# print(updateLink(url))\n",
    "\n",
    "addLinks(url)\n",
    "addAllLinks(updateLink(url))\n",
    "\n",
    "\n",
    "# print(len(links))\n",
    "# addLinks(url)\n",
    "# print(updateLink(url))\n",
    "# print(len(links))\n",
    "# addLinks(updateLink(url))\n",
    "\n",
    "# print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "filename = r\"C:\\Users\\John Kim\\Desktop\\kdata.csv\"\n",
    "\n",
    "# adds all links into a row \"link\"\n",
    "with open(filename, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"link\"])\n",
    "    for link in links:\n",
    "        writer.writerow([link])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clears the csv file\n",
    "f = open(filename, \"w+\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/49231-move-to-heaven\">Move to Heaven</a>\n",
      "<a href=\"/36269-doctor-playbook\">Hospital Playlist</a>\n",
      "<a href=\"/54625-flower-of-evil\">Flower of Evil</a>\n",
      "<a href=\"/57173-hospital-playlist-2\">Hospital Playlist 2</a>\n",
      "<a href=\"/25172-my-ajusshi\">My Mister</a>\n",
      "<a href=\"/22624-wise-prison-life\">Prison Playbook</a>\n",
      "<a href=\"/13544-reply-1988\">Reply 1988</a>\n",
      "<a href=\"/58365-queen-cheorin\">Mr. Queen</a>\n",
      "<a href=\"/21308-mother\">Mother</a>\n",
      "<a href=\"/49865-psycho-but-it-s-okay\">It's Okay to Not Be Okay</a>\n",
      "<a href=\"/13239-signal\">Signal</a>\n",
      "<a href=\"/35729-emergency-lands-of-love\">Crash Landing on You</a>\n",
      "<a href=\"/59381-navillera\">Navillera</a>\n",
      "<a href=\"/61371-vincenzo\">Vincenzo</a>\n",
      "<a href=\"/690709-happiness\">Happiness</a>\n",
      "<a href=\"/32220-kingdom-2\">Kingdom: Season 2</a>\n",
      "<a href=\"/23920-mr.-sunshine\">Mr. Sunshine</a>\n",
      "<a href=\"/30740-sky-castle\">SKY Castle</a>\n",
      "<a href=\"/10814-healer\">Healer</a>\n",
      "<a href=\"/61399-dress-sleeved-red\">The Red Sleeve</a>\n"
     ]
    }
   ],
   "source": [
    "testLinks = []\n",
    "\n",
    "response = requests.get('https://mydramalist.com/search?adv=titles&ty=68&co=3&st=3&so=top&page=1') # sends and get HTTP response\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "kdramaSites = soup.select('div > div.col-xs-9.row-cell.content > h6 > a:nth-child(1)') # list of all items with this selector\n",
    "\n",
    "for site in kdramaSites:\n",
    "    print(site)\n",
    "    testLinks.append(\"https://mydramalist.com\" + site.get('href'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Move to Heaven', 'Hospital Playlist', 'Flower of Evil', 'Hospital Playlist 2', 'My Mister', 'Prison Playbook', 'Reply 1988', 'Mr. Queen', 'Mother', \"It's Okay to Not Be Okay\", 'Signal', 'Crash Landing on You', 'Navillera', 'Vincenzo', 'Happiness', 'Kingdom: Season 2', 'Mr. Sunshine', 'SKY Castle', 'Healer', 'The Red Sleeve']\n",
      "['10', '12', '16', '12', '16', '16', '20', '20', '16', '16', '16', '16', '12', '20', '12', '6', '24', '20', '20', '17']\n",
      "['52', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '45', '1', '1', '60', '1']\n",
      "['9.2', '9.1', '9.1', '9.1', '9.1', '9.0', '9.0', '9.0', '9.0', '9.0', '9.0', '9.0', '9.0', '9.0', '8.9', '8.9', '8.9', '8.9', '8.9', '8.9']\n",
      "['18+ Restricted (violence & profanity)', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '18+ Restricted (violence & profanity)', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older', '15+ - Teens 15 or older']\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "episodes = []\n",
    "avgDuration = []\n",
    "scores = []\n",
    "contentRating = []\n",
    "\n",
    "# gets titles, episodes, average duration, scores (x/10), and content rating from each link\n",
    "for link in testLinks:\n",
    "    response = requests.get(link) # sends and get HTTP response\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # select makes it a list bruhhhh\n",
    "    kdramaTitle = soup.select('#content > div > div.container-fluid.title-container > div > div.col-lg-4.col-md-4 > div > div:nth-child(2) > div.box-body.light-b > ul > li:nth-child(1) > span') # list of all items with this selector\n",
    "    for title in kdramaTitle:\n",
    "        titles.append(title.text)\n",
    "\n",
    "    kdramaEpisode = soup.select('#show-detailsxx > div.show-detailsxss > ul.list.m-a-0.hidden-md-up > li:nth-child(3)')\n",
    "    for episode in kdramaEpisode:\n",
    "        epNum = episode.text.split(\" \")[1]\n",
    "        episodes.append(epNum)\n",
    "\n",
    "    kdramaDuration = soup.select(\"#show-detailsxx > div.show-detailsxss > ul.list.m-a-0.hidden-md-up > li:nth-child(7)\")\n",
    "    for duration in kdramaDuration:\n",
    "        epDuration = duration.text[10:]\n",
    "        avgDuration.append(epDuration[1])\n",
    "\n",
    "    kdramaScore = soup.select(\"#show-detailsxx > div.show-detailsxss > ul.list.m-a-0.hidden-md-up > li:nth-child(8)\")\n",
    "    for score in kdramaScore:\n",
    "        epScore = score.text.split(\" \")[1]\n",
    "        scores.append(epScore)\n",
    "\n",
    "    kdramaRating = soup.select(\"#show-detailsxx > div.show-detailsxss > ul.list.m-a-0.hidden-md-up > li:nth-child(11)\")\n",
    "    for rating in kdramaRating:\n",
    "        epRating = rating.text[16:]\n",
    "        contentRating.append(epRating)\n",
    "\n",
    "print(titles)\n",
    "print(episodes)\n",
    "print(avgDuration)\n",
    "print(scores)\n",
    "print(contentRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "607d36cc386bb86783cd5b11618343f80a7452933a7ad3095adf519582379640"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
